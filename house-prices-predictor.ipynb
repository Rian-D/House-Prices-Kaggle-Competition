{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:51:41.847394Z","iopub.execute_input":"2025-06-20T13:51:41.847626Z","iopub.status.idle":"2025-06-20T13:51:41.861584Z","shell.execute_reply.started":"2025-06-20T13:51:41.847597Z","shell.execute_reply":"2025-06-20T13:51:41.860705Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/home-data-for-ml-course/sample_submission.csv\n/kaggle/input/home-data-for-ml-course/sample_submission.csv.gz\n/kaggle/input/home-data-for-ml-course/train.csv.gz\n/kaggle/input/home-data-for-ml-course/data_description.txt\n/kaggle/input/home-data-for-ml-course/test.csv.gz\n/kaggle/input/home-data-for-ml-course/train.csv\n/kaggle/input/home-data-for-ml-course/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:51:58.593371Z","iopub.execute_input":"2025-06-20T13:51:58.594019Z","iopub.status.idle":"2025-06-20T13:52:02.813889Z","shell.execute_reply.started":"2025-06-20T13:51:58.593992Z","shell.execute_reply":"2025-06-20T13:52:02.813068Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Read data\ntrain_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n\n# Feature engineering function\ndef add_features(df):\n    df = df.copy()\n    \n    # Total square footage\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    \n    # Total bathrooms\n    df['TotalBaths'] = (df['FullBath'] + 0.5 * df['HalfBath'] + \n                       df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath'])\n    \n    # Age of house\n    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n    \n    # Years since remodel\n    df['YearsSinceRemodel'] = df['YrSold'] - df['YearRemodAdd']\n    \n    # Total porch area\n    df['TotalPorchSF'] = (df['OpenPorchSF'] + df['EnclosedPorch'] + \n                         df['3SsnPorch'] + df['ScreenPorch'])\n    \n    # Has pool (binary feature)\n    df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n    \n    # Has garage (binary feature)\n    df['HasGarage'] = (df['GarageArea'] > 0).astype(int)\n    \n    # Has basement (binary feature)\n    df['HasBsmt'] = (df['TotalBsmtSF'] > 0).astype(int)\n    \n    # Has fireplace (binary feature)\n    df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n    \n    # Overall quality * overall condition interaction\n    df['OverallGrade'] = df['OverallQual'] * df['OverallCond']\n    \n    return df\n\n# Apply feature engineering to both datasets\ntrain_df_enhanced = add_features(train_df)\ntest_df_enhanced = add_features(test_df)\n\n# Split data FIRST to avoid leakage\nX_train, X_test, y_train, y_test = train_test_split(\n    train_df_enhanced.drop('SalePrice', axis=1),\n    train_df_enhanced['SalePrice'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Feature selection\ncategorical_cols = [cname for cname in X_train.columns\n                    if X_train[cname].dtype == \"object\" \n                    and cname != 'Id']  # Include ALL categorical features\n\nnumerical_cols = [cname for cname in X_train.columns\n                  if X_train[cname].dtype in ['int64', 'float64']\n                  and cname != 'Id']  # Include ALL numerical features\n\ncols = categorical_cols + numerical_cols\n\nprint(f\"Selected {len(categorical_cols)} categorical features: {categorical_cols[:5]}...\")\nprint(f\"Selected {len(numerical_cols)} numerical features: {numerical_cols[:5]}...\")\nprint(f\"Total features: {len(cols)}\")\n\n# Processed data\ntrain = X_train[cols].copy()\nvalid = X_test[cols].copy()\ntest = test_df_enhanced[cols].copy() \n\nnumerical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\n# Model pipeline\nmodel = XGBRegressor(random_state=42)\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Hyperparameter tuning\nparam_dist = {\n    'model__n_estimators': randint(100, 1000),\n    'model__learning_rate': uniform(0.01, 0.19),\n    'model__max_depth': randint(3, 9),\n    'model__reg_alpha': uniform(0, 10),\n    'model__reg_lambda': uniform(1, 100),\n    'model__subsample': uniform(0.7, 0.3),\n    'model__colsample_bytree': uniform(0.7, 0.3),\n}\n\nrandom_search = RandomizedSearchCV(\n    pipeline,\n    param_distributions=param_dist,\n    n_iter=100,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42,\n    error_score='raise'\n)\n\n# Fit and validate\nrandom_search.fit(train, y_train)\nbest_model = random_search.best_estimator_\nval_predictions = best_model.predict(valid)\nval_mae = mean_absolute_error(y_test, val_predictions)\nprint(\"Validation MAE:\", val_mae)\n\n# Final submission\ntest_predictions = best_model.predict(test)\noutput = pd.DataFrame({'Id': test_df[\"Id\"], 'SalePrice': test_predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T13:54:54.420540Z","iopub.execute_input":"2025-06-20T13:54:54.420912Z","iopub.status.idle":"2025-06-20T14:04:13.135897Z","shell.execute_reply.started":"2025-06-20T13:54:54.420887Z","shell.execute_reply":"2025-06-20T14:04:13.135037Z"}},"outputs":[{"name":"stdout","text":"Selected 43 categorical features: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour']...\nSelected 46 numerical features: ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond']...\nTotal features: 89\nFitting 5 folds for each of 100 candidates, totalling 500 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"Validation MAE: 15797.611488655823\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Read data\ntrain_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n\n# Clean and split\ntrain_df.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train_df['SalePrice']\ntrain_df.drop(['SalePrice'], axis=1, inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, train_size=0.8, test_size=0.2, random_state=42)\n\n# Select features\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in X_train.columns if \n                  X_train[cname].dtype in ['int64', 'float64']]\n\ncols = categorical_cols + numerical_cols\n\ntrain = X_train[cols].copy()\nvalid = X_test[cols].copy()\ntest = test_df.copy()  # âœ… don't drop columns here\n\n# Pipelines\nnumerical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\n# Model pipeline\nmodel = XGBRegressor(random_state=42)\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Hyperparameter tuning\nparam_dist = {\n    'model__n_estimators': randint(100, 1000),\n    'model__learning_rate': uniform(0.01, 0.19),\n    'model__max_depth': randint(3, 9),\n    'model__reg_alpha': uniform(0, 10),\n    'model__reg_lambda': uniform(1, 100),\n    'model__subsample': uniform(0.7, 0.3),\n    'model__colsample_bytree': uniform(0.7, 0.3),\n}\n\nrandom_search = RandomizedSearchCV(\n    pipeline,\n    param_distributions=param_dist,\n    n_iter=100,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42,\n    error_score='raise'\n)\n\n# Fit model\nrandom_search.fit(train, y_train)\n\n# Evaluate\nbest_model = random_search.best_estimator_\nval_predictions = best_model.predict(valid)\nval_mae = mean_absolute_error(y_test, val_predictions)\nprint(\"Validation MAE:\", val_mae)\n\n# Predict on Kaggle test set\ntest_predictions = best_model.predict(test)\n\n# Create submission\noutput = pd.DataFrame({'Id': test_df[\"Id\"], 'SalePrice': test_predictions})\noutput.to_csv('submission5.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:38:10.721667Z","iopub.execute_input":"2025-06-16T17:38:10.722125Z","iopub.status.idle":"2025-06-16T17:45:14.464865Z","shell.execute_reply.started":"2025-06-16T17:38:10.722099Z","shell.execute_reply":"2025-06-16T17:45:14.464123Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 100 candidates, totalling 500 fits\nValidation MAE: 16943.706897474316\n","output_type":"stream"}],"execution_count":4}]}