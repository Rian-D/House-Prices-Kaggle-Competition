{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:32:03.014044Z","iopub.execute_input":"2025-06-21T17:32:03.014412Z","iopub.status.idle":"2025-06-21T17:32:03.026455Z","shell.execute_reply.started":"2025-06-21T17:32:03.014374Z","shell.execute_reply":"2025-06-21T17:32:03.025658Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/home-data-for-ml-course/sample_submission.csv\n/kaggle/input/home-data-for-ml-course/sample_submission.csv.gz\n/kaggle/input/home-data-for-ml-course/train.csv.gz\n/kaggle/input/home-data-for-ml-course/data_description.txt\n/kaggle/input/home-data-for-ml-course/test.csv.gz\n/kaggle/input/home-data-for-ml-course/train.csv\n/kaggle/input/home-data-for-ml-course/test.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:32:05.235484Z","iopub.execute_input":"2025-06-21T17:32:05.235821Z","iopub.status.idle":"2025-06-21T17:32:10.071316Z","shell.execute_reply.started":"2025-06-21T17:32:05.235796Z","shell.execute_reply":"2025-06-21T17:32:10.070527Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#best: 14941","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read data\ntrain_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n\n# Feature engineering function\ndef add_features(df):\n    df = df.copy()\n    \n    # Total square footage\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    \n    # Total bathrooms\n    df['TotalBaths'] = (df['FullBath'] + 0.5 * df['HalfBath'] + \n                       df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath'])\n    \n    # Age of house\n    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n    \n    # Years since remodel\n    df['YearsSinceRemodel'] = df['YrSold'] - df['YearRemodAdd']\n    \n    # Total porch area\n    df['TotalPorchSF'] = (df['OpenPorchSF'] + df['EnclosedPorch'] + \n                         df['3SsnPorch'] + df['ScreenPorch'])\n    \n    # Has pool (binary feature)\n    df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n    \n    # Has garage (binary feature)\n    df['HasGarage'] = (df['GarageArea'] > 0).astype(int)\n    \n    # Has basement (binary feature)\n    df['HasBsmt'] = (df['TotalBsmtSF'] > 0).astype(int)\n    \n    # Has fireplace (binary feature)\n    df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n    \n    # Overall quality * overall condition interaction\n    df['OverallGrade'] = df['OverallQual'] * df['OverallCond']\n\n    # 3 neigborhoods with highest mean saleprice\n    df['ExpensiveArea'] = df['Neighborhood'].isin([\"NoRidge\", \"StoneBr\", \"NridgHt\"]).astype(int) \n\n    df['TotalCond'] = (df['Condition1'] != \"Norm\").astype(int) + (df['Condition2'] != \"Norm\").astype(int)\n\n    mapping = {\n    'Ex': 5,  \n    'Gd': 4,  \n    'TA': 3,  \n    'Fa': 2,  \n    'Po': 1,\n    'NA': 0,\n    }\n\n    df['ExterGrade'] = df['ExterCond'].fillna('NA').map(mapping) * df['ExterQual'].fillna('NA').map(mapping)\n    df['BsmtGrade'] = df['BsmtCond'].fillna('NA').map(mapping) * df['BsmtQual'].fillna('NA').map(mapping)\n    df['GarageGrade'] = df['GarageCond'].fillna('NA').map(mapping) * df['GarageQual'].fillna('NA').map(mapping) \n    return df\n\n# Apply feature engineering to both datasets\ntrain_df_enhanced = add_features(train_df)\ntest_df_enhanced = add_features(test_df)\n\n# Split data FIRST to avoid leakage\nX_train, X_test, y_train, y_test = train_test_split(\n    train_df_enhanced.drop('SalePrice', axis=1),\n    train_df_enhanced['SalePrice'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Feature selection\ncategorical_cols = [cname for cname in X_train.columns\n                    if X_train[cname].dtype == \"object\" \n                    and cname != 'Id']  # Include ALL categorical features\n\nnumerical_cols = [cname for cname in X_train.columns\n                  if X_train[cname].dtype in ['int64', 'float64']\n                  and cname != 'Id']  # Include ALL numerical features\n\ncols = categorical_cols + numerical_cols\n\nprint(f\"Selected {len(categorical_cols)} categorical features: {categorical_cols[:5]}...\")\nprint(f\"Selected {len(numerical_cols)} numerical features: {numerical_cols[:5]}...\")\nprint(f\"Total features: {len(cols)}\")\n\n# Processed data\ntrain = X_train[cols].copy()\nvalid = X_test[cols].copy()\ntest = test_df_enhanced[cols].copy() \n\nnumerical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\n# Model pipeline\nmodel = XGBRegressor(random_state=42)\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Hyperparameter tuning\nparam_dist = {\n    'model__n_estimators': randint(100, 1000),\n    'model__learning_rate': uniform(0.01, 0.19),\n    'model__max_depth': randint(3, 9),\n    'model__reg_alpha': uniform(0, 10),\n    'model__reg_lambda': uniform(1, 100),\n    'model__subsample': uniform(0.7, 0.3),\n    'model__colsample_bytree': uniform(0.7, 0.3),\n}\n\nrandom_search = RandomizedSearchCV(\n    pipeline,\n    param_distributions=param_dist,\n    n_iter=100,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42,\n    error_score='raise'\n)\n\n# Fit and validate\nrandom_search.fit(train, y_train)\nbest_model = random_search.best_estimator_\nval_predictions = best_model.predict(valid)\nval_mae = mean_absolute_error(y_test, val_predictions)\nprint(\"Validation MAE:\", val_mae)\n\n# Final submission\ntest_predictions = best_model.predict(test)\noutput = pd.DataFrame({'Id': test_df[\"Id\"], 'SalePrice': test_predictions})\noutput.to_csv('submission10.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check average prices by neighborhood to confirm\ntrain_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntrain_df.groupby('Neighborhood')['SalePrice'].mean().sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:32:15.236441Z","iopub.execute_input":"2025-06-21T17:32:15.236890Z","iopub.status.idle":"2025-06-21T17:32:15.306866Z","shell.execute_reply.started":"2025-06-21T17:32:15.236864Z","shell.execute_reply":"2025-06-21T17:32:15.305985Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Neighborhood\nNoRidge    335295.317073\nNridgHt    316270.623377\nStoneBr    310499.000000\nTimber     242247.447368\nVeenker    238772.727273\nSomerst    225379.837209\nClearCr    212565.428571\nCrawfor    210624.725490\nCollgCr    197965.773333\nBlmngtn    194870.882353\nGilbert    192854.506329\nNWAmes     189050.068493\nSawyerW    186555.796610\nMitchel    156270.122449\nNAmes      145847.080000\nNPkVill    142694.444444\nSWISU      142591.360000\nBlueste    137500.000000\nSawyer     136793.135135\nOldTown    128225.300885\nEdwards    128219.700000\nBrkSide    124834.051724\nBrDale     104493.750000\nIDOTRR     100123.783784\nMeadowV     98576.470588\nName: SalePrice, dtype: float64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Read data\ntrain_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n\n# Clean and split\ntrain_df.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train_df['SalePrice']\ntrain_df.drop(['SalePrice'], axis=1, inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, train_size=0.8, test_size=0.2, random_state=42)\n\n# Select features\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in X_train.columns if \n                  X_train[cname].dtype in ['int64', 'float64']]\n\ncols = categorical_cols + numerical_cols\n\ntrain = X_train[cols].copy()\nvalid = X_test[cols].copy()\ntest = test_df.copy()  # âœ… don't drop columns here\n\n# Pipelines\nnumerical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\n# Model pipeline\nmodel = XGBRegressor(random_state=42)\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n# Hyperparameter tuning\nparam_dist = {\n    'model__n_estimators': randint(100, 1000),\n    'model__learning_rate': uniform(0.01, 0.19),\n    'model__max_depth': randint(3, 9),\n    'model__reg_alpha': uniform(0, 10),\n    'model__reg_lambda': uniform(1, 100),\n    'model__subsample': uniform(0.7, 0.3),\n    'model__colsample_bytree': uniform(0.7, 0.3),\n}\n\nrandom_search = RandomizedSearchCV(\n    pipeline,\n    param_distributions=param_dist,\n    n_iter=100,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42,\n    error_score='raise'\n)\n\n# Fit model\nrandom_search.fit(train, y_train)\n\n# Evaluate\nbest_model = random_search.best_estimator_\nval_predictions = best_model.predict(valid)\nval_mae = mean_absolute_error(y_test, val_predictions)\nprint(\"Validation MAE:\", val_mae)\n\n# Predict on Kaggle test set\ntest_predictions = best_model.predict(test)\n\n# Create submission\noutput = pd.DataFrame({'Id': test_df[\"Id\"], 'SalePrice': test_predictions})\noutput.to_csv('submission5.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T17:38:10.721667Z","iopub.execute_input":"2025-06-16T17:38:10.722125Z","iopub.status.idle":"2025-06-16T17:45:14.464865Z","shell.execute_reply.started":"2025-06-16T17:38:10.722099Z","shell.execute_reply":"2025-06-16T17:45:14.464123Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 100 candidates, totalling 500 fits\nValidation MAE: 16943.706897474316\n","output_type":"stream"}],"execution_count":4}]}